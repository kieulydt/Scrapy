{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nội dung chính "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Giới thiệu\n",
    "2. Cài đặt\n",
    "3. Khái niệm chính <br/>\n",
    "    3.1: Command line <br/>\n",
    "    3.2: Spiders <br/>\n",
    "    3.3: Selectors <br/>\n",
    "    3.4: Request và Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrapy là gì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy là một framework được viết bằng Python, nó cấp sẵn 1 cấu trúc tương đối hoàn chỉnh để thực hiện việc crawl và extract data từ website một cách nhanh chóng và dễ dàng. Ví dụ như lấy toàn bộ hình ảnh trên 1 website; các bài viết trên các trang báo; thông tin dữ liệu các sản phẩm moblie, ô tô; các thông tin public trên facebook;..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Người dùng chỉ cần bổ sung thêm định nghĩa về dữ liệu cần lấy là xong, ví dụ như URL bắt đầu là gì, link chuyển qua trang mới, các thông tin cần lấy ở mỗi trang là gì..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cài đặt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy chạy trên Python 2.7 và Python 3.5 trở lên trong CPython và PyPy 5.9 trở lên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như bạn sử dụng Anaconda hoặc Miniconda, bạn có thể cài đặt gói từ conda-forge channel. Để cài đặt Scrapy sử dụng `conda`, chạy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conda install -c conda-forge scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài ra, nếu bạn đã quen với việc cài đặt các gói Python, bạn có thể cài đặt Scrapy từ PyPI với:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install Scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Các khái niệm cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Command line tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy được điều khiển thông qua command-line tool. Scrapy cung cấp một số lệnh, với nhiều mục đích thì sẽ sử dụng các nhóm lệnh khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là cấu trúc thư mục khi vừa tạo thông qua scrapy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scraper/\n",
    "├── scraper                         # nơi chứa code của dự án\n",
    "│   ├── __init__.py\n",
    "│   ├── items.py                    # nơi định nghĩa các trường dữ liệu cần lưu vào db\n",
    "│   ├── pipelines.py                # nơi xử lý các item trích xuất được và lưu vào db\n",
    "│   ├── settings.py                 # cấu hình thêm các phần mở rộng (middlewares) và các thông số cấu hình khác\n",
    "│   └── spiders                     # thư mục chứa các spider\n",
    "│       └── __init__.py\n",
    "└── scrapy.cfg                      # file cấu hình về deploy và settings của project`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một số lệnh command-line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"commandline.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ về sử dụng scrapy shell. Đầu tiên mở cmd chạy command `scrapy shell [url]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"shell.PNG\">\n",
    "<br/>\n",
    "<img src=\"shell1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó có thể trực tiếp sử dụng các Selector để trích xuất trực tiếp, thực hiện điều này giúp kiểm tra một cách trực quan hơn. Ví dụ ta muốn tách title của bài báo, nhập `response.css('"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"shell2.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiders là class được viết bởi người dùng, xác định một hoặc một nhóm trang web nhất định sẽ được quét, bao gồm cách thực hiện thu thập thông tin và cách trích xuất dữ liệu có cấu trúc từ các trang web đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối với Spiders, quá trình thu thập dữ liệu có chu trình như sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bạn bắt đầu bằng cách tạo các yêu cầu bạn đầu để thu thập dữ liệu từ URL đầu tiên và chỉ định chức năng gọi lại được gọi với phản hồi từ các yêu cầu đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Trong hàm gọi lại, bạn phân tích cú pháp phản hồi và trả về các ký tự với dữ liệu được trích xuất.  Những yêu cầu đó cũng có thể chứa một yêu cầu gọi lại, sau đó cũng sẽ được Scrapy duyệt và phản hồi lại yêu cầu đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cuối cùng, các mục được trả về từ spiders sẽ được duy trì trong CSDL hoặc đươc ghi vào một tệp bằng cách sử dụng Feed exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### items.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chứa các định nghĩa thông tin mình muốn extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Item, Field\n",
    "\n",
    "class VNExpressItem(Item):\n",
    "    # define the fields for your item here like:\n",
    "    title = Field()\n",
    "    url = Field()\n",
    "    content = Field()\n",
    "    author = Field()\n",
    "    description = Field()\n",
    "    publish_date = Field()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy.Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Định nghĩa cách chuyển giữa các trang và cách lấy thông tin. ví dụ định nghĩa domain thực hiện việc scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một số phương thức và thuộc tính chính:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"method_scrapy.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ: Crawl dữ liệu trên báo vnexpress \n",
    "<br/>\n",
    "<a href=\"https://vnexpress.net/du-lich/viet-nam-huong-den-bau-troi-mo-asean-4001184.html\">link báo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước tiên ta tạo một file myspider.py trong thư mục spiders đã được tạo. Thư mục này là nơi chúng ta đưa ra các chỉ định cho Scrapy biết chính xác chúng ta muốn thu thập dữ liệu gì. Trong thư mục này các bạn có thể định nghĩa các spiders khác nhau cho các trang web khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"vnexpress\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = ['https://vnexpress.net/du-lich/viet-nam-huong-den-bau-troi-mo-asean-4001184.html']\n",
    "        for url in urls:\n",
    "            #Phương thức call back gọi lại hàm parse_article\n",
    "            #hàm parse_article sẽ xử lý phản hồi đã được tải khi request url \n",
    "            request = Request(url=url, callback=self.parse_artilce) \n",
    "            request.meta['url'] = url\n",
    "            yield request\n",
    "\n",
    "    def parse_artilce(self, response):\n",
    "        title = response.xpath('//*[@id=\"col_sticky\"]/h1/text()').extract()[0].strip()\n",
    "        description = response.xpath('//*[@id=\"col_sticky\"]/p[1]').extract()[0].strip()\n",
    "        content = response.xpath('//*[@id=\"col_sticky\"]/article').extract()[0].strip()\n",
    "        author = response.xpath('//*[@id=\"col_sticky\"]/p[2]/strong/text()').extract()[0].strip()\n",
    "        publish_date = response.xpath('//*[@id=\"col_sticky\"]/header/span/text()').extract()[0].strip()\n",
    "        \n",
    "        item = VNExpressItem()\n",
    "        item['title'] = title\n",
    "        item['url'] = response.meta['url']\n",
    "        item['content'] = content\n",
    "        item['description'] = description\n",
    "        item['author'] = author\n",
    "        item['publish_date'] = publish_date\n",
    "        \n",
    "        yield item\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó sẽ chạy lệnh ở command line để thực hiện lấy các thông tin mà spider trích xuất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl vnexpress`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây sử dụng request.meta và response.meta, trong đó response.meta trả về giá trị lưu ở request.meta trước khi request url.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để Insert các kết quả tìm được vào database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi quét các trang web, tác vụ phổ biến nhất cần thực hiện là trích xuất dữ liệu từ nguồn HTML. Scrapy có cơ chế riêng để trích xuất dữ liệu. Chúng chọn một số thành phần nhất định của tài liệu HTML được chỉ định bởi các biểu thức XPath hoặc CSS. XPath để chọn các nút trong tài liệu XML, cũng có thể được sử dụng cho các tài liệu HTML. CSS là ngôn ngữ để áp dụng các kiểu cho các tài liệu HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truy vấn các phản hồi bằng XPath và CSS ta sử dụng: `response.xpath()` và `response.css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ:\n",
    "<br/>\n",
    "`>>> response.xpath('//span/text()').get()`\n",
    "<br/>\n",
    "'good'\n",
    "<br/>\n",
    "`>>>response.css('span::text').get()`\n",
    "<br/>\n",
    "'good'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Một số phương thức"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"method_extract.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "sel = Selector(text=\"\"\"\n",
    "<html>\n",
    " <head>\n",
    "  <base href='http://example.com/' />\n",
    "  <title>Example website</title>\n",
    " </head>\n",
    " <body>\n",
    "  <div id='images'>\n",
    "   <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' /></a>\n",
    "   <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' /></a>\n",
    "   <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' /></a>\n",
    "   <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' /></a>\n",
    "   <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' /></a>\n",
    "  </div>\n",
    " </body>\n",
    "</html>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Chú ý:</b> Khi thực hiện crawl trên web thì phần text của response chính là mã nguồn đc scrapy tải xuống  hay nói cách khác response = sel. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>title::text</b>: Trích chọn text trong thẻ title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example website'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css('title::text').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*::text</b> chọn tất cả text ở trong các thẻ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n   ',\n",
       " 'Name: My image 1 ',\n",
       " '\\n   ',\n",
       " 'Name: My image 2 ',\n",
       " '\\n   ',\n",
       " 'Name: My image 3 ',\n",
       " '\\n   ',\n",
       " 'Name: My image 4 ',\n",
       " '\\n   ',\n",
       " 'Name: My image 5 ',\n",
       " '\\n  ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css('#images *::text').getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>foo::text</b> trả về kết quả không nếu phần tử foo tồn tại nhưng không chứa text.\n",
    "<br/>\n",
    "Sử dụng <b>default=''</b> nếu bạn muốn trả về string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css('img::text').getall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css('img::text').get(default='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>a::attr(href)</b> chọn giá trị thuộc tính href của links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css('a::attr(href)').getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể chọn lọc ra một số thành phần chính xác trên một trang web bằng cách sử dụng XPath. Chúng ta có thể lấy XPath dễ dàng bằng tool hay sử dụng Chrome để tìm XPath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ chúng ta sẽ lấy XPath trên stack overflow. Chọn câu hỏi đầu tiên và chọn \"Inspect Element\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"xpath.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ, chúng ta lấy XPath của phần tử đầu tiên `<div class=\"summary\">`, kết quả sẽ tương tự `//*[@id=\"question-summary-34194623\"]/div[2]`. Chúng ta sẽ test trong Javascript console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"xpath1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4: Request và Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy sử dụng đối tượng Request và Response để thu thập dữ liệu websites.<br/>\n",
    "Thông thường các đối tượng Request được tạo trong spider và truyền qua hệ thống cho đến khi đến trình download, thực thi yêu cầu và trả về đối tượng Response di chuyển trở lại spider đã gửi Request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
